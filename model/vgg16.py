"""VGG16_PyTorch.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1ISX0xARf8YgyX7LzC7aQRAGF_vjPPpZ6
"""

import torch
import torch.nn.functional as F
import torch.nn as nn

class VGG16(torch.nn.Module):
    def __init__(self,num_classes=1000,):
        super(VGG16, self).__init__()  
        #conv1
        layer1 = torch.nn.Sequential()
        layer1.add_module('conv1', torch.nn.Conv2d(3, 64, 3, 1, padding=1))
        layer1.add_module('batch_norm1', torch.nn.BatchNorm2d(64))
        layer1.add_module('relu1', torch.nn.ReLU(True))
        layer1.add_module('conv2', torch.nn.Conv2d(64, 64, 3, 1, padding=1))
        layer1.add_module('batch_norm2', torch.nn.BatchNorm2d(64))
        layer1.add_module('relu2', torch.nn.ReLU(True))
        layer1.add_module('pool', torch.nn.MaxPool2d(2, 2))
        self.layer1 = layer1

        #conv2
        layer2 = torch.nn.Sequential()
        layer2.add_module('conv1', torch.nn.Conv2d(64, 128, 3, 1, padding=1))
        layer2.add_module('batch_norm1', torch.nn.BatchNorm2d(128))
        layer2.add_module('relu1', torch.nn.ReLU(True))
        layer2.add_module('conv2', torch.nn.Conv2d(128, 128, 3, 1, padding=1))
        layer2.add_module('batch_norm2', torch.nn.BatchNorm2d(128))
        layer2.add_module('relu2', torch.nn.ReLU(True))
        layer2.add_module('pool', torch.nn.MaxPool2d(2, 2))
        self.layer2 = layer2

        #conv3
        layer3 = torch.nn.Sequential()
        layer3.add_module('conv1', torch.nn.Conv2d(128, 256, 3, 1, padding=1))
        layer3.add_module('batch_norm1', torch.nn.BatchNorm2d(256))
        layer3.add_module('relu1', torch.nn.ReLU(True))
        layer3.add_module('conv2', torch.nn.Conv2d(256, 256, 3, 1, padding=1))
        layer3.add_module('batch_norm2', torch.nn.BatchNorm2d(256))
        layer3.add_module('relu2', torch.nn.ReLU(True))
        layer3.add_module('conv3', torch.nn.Conv2d(256, 256, 3, 1, padding=1))
        layer3.add_module('batch_norm3', torch.nn.BatchNorm2d(256))
        layer3.add_module('relu3', torch.nn.ReLU(True))
        layer3.add_module('pool', torch.nn.MaxPool2d(2, 2))
        self.layer3 = layer3

        #conv4
        layer4 = torch.nn.Sequential()
        layer4.add_module('conv1', torch.nn.Conv2d(256, 512, 3, 1, padding=1))
        layer4.add_module('batch_norm1', torch.nn.BatchNorm2d(512))
        layer4.add_module('relu1', torch.nn.ReLU(True))
        layer4.add_module('conv2', torch.nn.Conv2d(512, 512, 3, 1, padding=1))
        layer4.add_module('batch_norm2', torch.nn.BatchNorm2d(512))
        layer4.add_module('relu2', torch.nn.ReLU(True))
        layer4.add_module('conv3', torch.nn.Conv2d(512, 512, 3, 1, padding=1))
        layer4.add_module('batch_norm3', torch.nn.BatchNorm2d(512))
        layer4.add_module('relu3', torch.nn.ReLU(True))
        layer4.add_module('pool', torch.nn.MaxPool2d(2, 2))
        self.layer4 = layer4

        #conv5
        layer5 = torch.nn.Sequential()
        layer5.add_module('conv1', torch.nn.Conv2d(512, 512, 3, 1, padding=1))
        layer5.add_module('batch_norm1', torch.nn.BatchNorm2d(512))
        layer5.add_module('relu1', torch.nn.ReLU(True))
        layer5.add_module('conv2', torch.nn.Conv2d(512, 512, 3, 1, padding=1))
        layer5.add_module('batch_norm2', torch.nn.BatchNorm2d(512))
        layer5.add_module('relu2', torch.nn.ReLU(True))
        layer5.add_module('conv3', torch.nn.Conv2d(512, 512, 3, 1, padding=1))
        layer5.add_module('batch_norm3', torch.nn.BatchNorm2d(512))
        layer5.add_module('relu3', torch.nn.ReLU(True))
        layer5.add_module('pool', torch.nn.MaxPool2d(2, 2))
        self.layer5 = layer5

        #fc1
        layer6 = torch.nn.Sequential()
        layer6.add_module('fc', torch.nn.Linear(25088, 4096))
        layer6.add_module('batch_norm', torch.nn.BatchNorm1d(4096))
        self.layer6 = layer6

        #fc2
        layer7 = torch.nn.Sequential()
        layer7.add_module('fc', torch.nn.Linear(4096, 4096))
        layer7.add_module('batch_norm', torch.nn.BatchNorm1d(4096))
        self.layer7 = layer7

        #fc_out
        layer8 = torch.nn.Sequential()
        layer8.add_module('fc', torch.nn.Linear(4096, num_classes))
        self.layer8 = layer8

    def forward(self, x):
        conv1 = self.layer1(x)
        conv2 = self.layer2(conv1)
        conv3 = self.layer3(conv2)
        conv4 = self.layer4(conv3)
        conv5 = self.layer5(conv4)
        fc1_input = conv5.view(conv5.size(0), -1)
        fc_1 = self.layer6(fc1_input)
        fc_2 = self.layer7(fc_1)
        fc_out = self.layer8(fc_2)
        return fc_out

model = VGG16()
if torch.cuda.is_available():
    model.cuda()